\documentclass[12pt]{article} %{{{
\usepackage[margin=1in]{geometry}

% Figures
\usepackage{graphicx}
\graphicspath{{../../figs/}}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% abbreviations
\def\eg{e.g.,~}
\def\ie{i.e.,~}
\def\cf{cf.\ }
\def\viz{viz.\ }
\def\vs{vs.\ }

% Refs
\usepackage{biblatex}
\addbibresource{main.bib}

\usepackage{url}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
%\newcommand{\eqnref}[1]{\eqref{eq:#1}}
%\newcommand{\thmref}[1]{Theorem~\ref{#1}}
%\newcommand{\prgref}[1]{Program~\ref{#1}}
%\newcommand{\algref}[1]{Algorithm~\ref{#1}}
%\newcommand{\clmref}[1]{Claim~\ref{#1}}
%\newcommand{\lemref}[1]{Lemma~\ref{#1}}
%\newcommand{\ptyref}[1]{Property~\ref{#1}}

% for quick author comments 
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{light-gray}{gray}{0.8}
\def\del#1{ {\color{light-gray}{#1}} }
\def\yy#1{\footnote{\color{red}\textbf{yy: #1}} }

%}}}

\begin{document} %{{{

\title{An awesome paper} %{{{
\date{\today}
\maketitle %}}}

\section{Introduction}\label{sec:introduction} %{{{


%}}}

\section{Results}\label{sec:results} %{{{ 


%}}}

\section{Methods}\label{sec:methods} %{{{
\subsection{Link prediction algorithms}

\textbf{Graph SAGE}
GraphSage is a neural network model that learns representations of nodes in graphs. Unlike GCNs that are implementing transductive learning, it uses an inductive setting that allows it to scale to large graphs. GraphSage employs a sampling-based approach to learn embeddings by aggregating information from a node's neighbors through a neighborhood aggregation operation. The final node representation is obtained by concatenating the aggregated neighborhood vector with the node's own features and passing it through a neural network.

Our study used GraphSage in an unsupervised learning setting since our data had no labels. The unsupervised training learns the parameters in a way that represents neighboring nodes closer to each other in the vector space. To achieve this, the loss function encourages neighboring nodes to have similar representations while keeping representations of two random nodes at a high distance.

To create feature vectors for nodes, we employed Laplacian Eigenmap embeddings as used in the GCN model. The GraphSage model has two hidden layers with 128 dimensions.

%}}}

\printbibliography{}
    
\end{document} %}}}
